---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r cache=TRUE}
#setwd("C:/Users/Robey/Desktop/MA478 Proj/MA478-Project")

library(dplyr)
library(ggplot2)
library(tidymodels)
library(stringr)
library(corrr)

start <- Sys.time()
data <- read.csv(file = "subset_used_cars_data.csv", stringsAsFactors = TRUE)
data <- data %>% 
  select(-c(X, vin, bed, city, dealer_zip, engine_type, bed_height, bed_length, 
            cabin, combine_fuel_economy, fleet, height, is_certified, 
            main_picture_url, vehicle_damage_category, width, description, 
            sp_name, sp_id, trimId, trim_name, wheel_system_display, listed_date, 
            exterior_color, interior_color, model_name, listing_id, sp_id, 
            major_options, make_name, franchise_make, )) %>% 
  mutate(back_legroom = extract_numeric(back_legroom),
         engine_cylinders = extract_numeric(engine_cylinders),
         front_legroom = extract_numeric(front_legroom),
         fuel_tank_volume = extract_numeric(fuel_tank_volume),
         length = extract_numeric(length),
         wheelbase = extract_numeric(wheelbase),
         power = as.numeric(gsub(".*?([0-9]+).*", "\\1", power)),
         torque = as.numeric(gsub(".*?([0-9]+).*", "\\1", torque)))
end <- Sys.time()

print(end-start)
```

### Exploratory Data Analysis
```{r}
data %>% 
  ggplot(aes(x = price)) +
  geom_histogram(bins = 80) +
  geom_vline(xintercept = max(data$price))
```
```{r}
data <- data %>% 
  filter(price <= mean(price) + 3*sd(price)) 

data%>% 
  ggplot(aes(x = price)) +
  geom_histogram(bins = 80) +
  geom_vline(xintercept = max(data$price))
```


```{r}
data %>% 
  correlate() %>% 
  rearrange() %>% 
  shave() %>% 
  rplot() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
#ggsave("plots/corrplot.png")
```
Variables that we would expect to be related show high levels of correlation.  Because our purposes are prediction (rather than inference), we can mostly ignore these relationships so long as the related variables improve our model performance.


#### Dealing with NA Values
```{r}
sapply(data, function(y) sum(length(which(is.na(y))))) %>% tidy() %>% 
  filter(x > 0) %>% 
  ggplot(aes(y = names, x = x)) +
  geom_histogram(stat = "identity", alpha = 0.6) +
  scale_x_log10("Number of Missing Values") +
  theme_minimal() +
  geom_label(aes(label = x), nudge_x = -.25) +
  ggtitle("Missing Value Counts")
#ggsave("plots/Cleaning_BeforeDropNA.png")
```
We do not wish to include observations that do not have data for `mileage` as these are likely incomplete listings, or listings that were not scraped correctly.

```{r}
data <- data %>% 
  drop_na(mileage) 

sapply(data, function(y) sum(length(which(is.na(y))))) %>% tidy() %>% 
  filter(x > 0) %>% 
  ggplot(aes(y = names, x = x)) +
  geom_histogram(stat = "identity", alpha = 0.6) +
  scale_x_continuous("Number of Missing Values") +
  theme_minimal() +
  geom_label(aes(label = x), nudge_x = -1) +
  ggtitle("Missing Value Counts After Dropping Listings With Missing Mileage")
#ggsave("plots/Cleaning_AfterDropNA.png")
```
However, even after removing these entries, there are still well over 7 thousand observations with at least one missing value.  To do this, we will use the `mice` package to perform multiple imputation using regression.

```{r}
library(mice)

start <- Sys.time()
imp <- mice(data, method = "cart", m = 1)
data <- complete(imp)
end <- Sys.time()
print(end-start)

data %>% write.csv("imputed_subset_used_cars.csv")
```

```{r}
sapply(data, function(y) sum(length(which(is.na(y))))) %>% tidy() %>% 
  filter(x > 0) 
```



```{r}
exploraory <- data %>% 
  sample_n(round(nrow(data)*0.25))

exploraory %>% 
  ggplot(aes(y = price, x = is_cpo, fill = is_cpo)) + 
  scale_y_log10() +
  geom_boxplot() + 
  coord_flip() +
  theme_minimal() +
  labs(title = "Price vs. Certified Pre-Owned",
       x = "", y = "Price($)", fill = "Is CPO")
#ggsave("plots/EDA_Price_CPO.png")

exploraory %>% 
  ggplot(aes(x = mileage, y = price, color = is_cpo)) +
  geom_point(alpha = 0.3) +
  theme_minimal() +
  scale_y_log10() +
  labs(title = "Price vs. Milage", 
       x = "Milage", y = "Price ($)", 
       color = "Is CPO")
#ggsave("plots/EDA_Milage_Price.png")

exploraory %>% 
  filter(owner_count <= 8) %>% 
  group_by(owner_count) %>% 
  ggplot(aes(x = owner_count, y = price, group = owner_count)) +
  geom_boxplot(alpha = 0.6) +
  theme_minimal() +
  scale_y_log10() +
  labs(title = "Price vs. Owner Count", 
       x = "Number of Previous Owners", y = "Price ($)", 
       fill = "Is CPO")
#ggsave("plots/EDA_NumOwners_Price.png")
```

### Train and Test Split/Pre-Process Data

Training and Validation Data
```{r Train Test}
set.seed(222)
# Put 3/4 of the data into the training set 
data_split <- initial_split(data, prop = 3/4)

# Create data frames for the two sets:
other_data <- training(data_split)
test_data  <- testing(data_split)

val_data <- validation_split(other_data, 
                             prop = 0.7)
```

Create Recipe for LASSO and Random Forest
```{r glm1_recipe}
lasso_recipe <- recipe(price ~ ., data = other_data) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors() - all_nominal())

summary(lasso_recipe)
```

### LASSO Regression
Add the LASSO to the workflow
```{r workflow}
lasso1 <- linear_reg(penalty = 0.1, mixture = 1) %>% # mixture = 1 -> lasso
  set_engine("glmnet")

workflow <- workflow() %>% 
  add_recipe(lasso_recipe)
```

#### Untuned LASSO
```{r}
lasso1_fit <- workflow %>% 
  add_model(lasso1) %>% 
  fit(data = other_data)

lasso1_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

lasso1_preds <- lasso1_fit %>% 
  augment(new_data = other_data)

lasso1_rsq <- rsq(lasso1_preds, price, .pred)$.estimate
```

#### Tuning $\lambda$ for LASSO

List of $\lambda$ values to test in CV
```{r penalty tuning LASSO}
lasso_2 <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

lambda_grid <- grid_regular(penalty(), levels = 100)
lambda_grid <- tibble(penalty = 10^seq(-4, 0, length.out = 40))

data_boot <- bootstraps(other_data)

doParallel::registerDoParallel()

lasso_grid <- tune_grid(
  workflow %>% 
    add_model(lasso_2),
  resamples = data_boot,
  grid = lambda_grid)

lasso_grid <- workflow %>% add_model(lasso_2) %>% 
  tune_grid(val_data,
            grid = lambda_grid,
            control = control_grid(save_pred = T, save_workflow = T),
            metrics = metric_set(rsq))

lasso_grid %>% 
  collect_metrics()
```

$\lambda$ selection
```{r Penalty tuning LASSO results}
lasso_grid %>% 
  collect_metrics() %>% 
  dplyr::filter(.metric == "rsq") %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  ylab("Explained Variance") +
  scale_x_log10(labels = scales::label_number()) + 
  ggtitle("Tuning Lasso Penalty") +
  theme_minimal() + theme_bw()
#ggsave("images/lasso_tuning.png")
```

```{r Select Best Penalty LASSO}
lasso_grid %>% 
  show_best("rsq", n = 15) %>% 
  arrange(-mean, penalty)

lr_best <- results %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  slice(1)
```

