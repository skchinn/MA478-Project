---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r read and filter data, cache=TRUE}
#setwd("C:/Users/Robey/Desktop/MA478 Proj/MA478-Project")

library(dplyr)
library(ggplot2)
library(tidymodels)
library(stringr)
library(corrr)
library(leaps)

start <- Sys.time()
data <- read.csv(file = "subset_used_cars_data.csv", stringsAsFactors = TRUE)
data <- data %>% 
  select(-c(X, vin, bed, city, dealer_zip, engine_type, bed_height, bed_length, 
            cabin, combine_fuel_economy, fleet, height, is_certified, 
            main_picture_url, vehicle_damage_category, width, description, 
            sp_name, sp_id, trimId, trim_name, wheel_system_display, listed_date, 
            exterior_color, interior_color, model_name, listing_id, sp_id, 
            major_options, make_name, franchise_make, horsepower)) %>% 
  mutate(back_legroom = extract_numeric(back_legroom),
         engine_cylinders = extract_numeric(engine_cylinders),
         front_legroom = extract_numeric(front_legroom),
         fuel_tank_volume = extract_numeric(fuel_tank_volume),
         length = extract_numeric(length),
         wheelbase = extract_numeric(wheelbase),
         power = as.numeric(gsub(".*?([0-9]+).*", "\\1", power)),
         torque = as.numeric(gsub(".*?([0-9]+).*", "\\1", torque)))
end <- Sys.time()

print(end-start)
```

### Exploratory Data Analysis
```{r initial price distribution}
data %>% 
  ggplot(aes(x = price)) +
  geom_histogram(bins = 80) +
  geom_vline(xintercept = max(data$price))
```
```{r removing price outliers}
data <- data %>% 
  filter(price <= mean(price) + 3*sd(price)) 

data%>% 
  ggplot(aes(x = price)) +
  geom_histogram(bins = 80) +
  geom_vline(xintercept = max(data$price))
```


```{r covariance plot}
data %>% 
  correlate() %>% 
  rearrange() %>% 
  shave() %>% 
  rplot() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
#ggsave("plots/corrplot.png")
```
Variables that we would expect to be related show high levels of correlation.  Because our purposes are prediction (rather than inference), we can mostly ignore these relationships so long as the related variables improve our model performance.


#### Dealing with NA Values
```{r initial NA counts}
sapply(data, function(y) sum(length(which(is.na(y))))) %>% tidy() %>% 
  filter(x > 0) %>% 
  ggplot(aes(y = names, x = x)) +
  geom_histogram(stat = "identity", alpha = 0.6) +
  scale_x_log10("Number of Missing Values") +
  theme_minimal() +
  geom_label(aes(label = x), nudge_x = -.25) +
  ggtitle("Missing Value Counts")
#ggsave("plots/Cleaning_BeforeDropNA.png")
```
We do not wish to include observations that do not have data for `mileage` as these are likely incomplete listings, or listings that were not scraped correctly.

```{r }
data <- data %>% 
  drop_na(mileage) 

sapply(data, function(y) sum(length(which(is.na(y))))) %>% tidy() %>% 
  filter(x > 0) %>% 
  ggplot(aes(y = names, x = x)) +
  geom_histogram(stat = "identity", alpha = 0.6) +
  scale_x_continuous("Number of Missing Values") +
  theme_minimal() +
  geom_label(aes(label = x), nudge_x = -1) +
  ggtitle("Missing Value Counts After Dropping Listings With Missing Mileage")
#ggsave("plots/Cleaning_AfterDropNA.png")
```
However, even after removing these entries, there are still well over 7 thousand observations with at least one missing value.  To do this, we will use the `mice` package to perform multiple imputation using regression.

```{r}
library(mice)

start <- Sys.time()
imp <- mice(data, method = "cart", m = 3)
data <- complete(imp)
end <- Sys.time()
print(end-start)

data %>% write.csv("imputed_subset_used_cars.csv")
```

```{r}
sapply(data, function(y) sum(length(which(is.na(y))))) %>% tidy() %>% 
  filter(x > 0) 
```



```{r}
exploraory <- data %>% 
  sample_n(round(nrow(data)*0.25))

exploraory %>% 
  ggplot(aes(y = price, x = is_cpo, fill = is_cpo)) + 
  scale_y_log10() +
  geom_boxplot() + 
  coord_flip() +
  theme_minimal() +
  labs(title = "Price vs. Certified Pre-Owned",
       x = "", y = "Price($)", fill = "Is CPO")
#ggsave("plots/EDA_Price_CPO.png")

exploraory %>% 
  ggplot(aes(x = mileage, y = price, color = is_cpo)) +
  geom_point(alpha = 0.3) +
  theme_minimal() +
  scale_y_log10() +
  labs(title = "Price vs. Milage", 
       x = "Milage", y = "Price ($)", 
       color = "Is CPO")
#ggsave("plots/EDA_Milage_Price.png")

exploraory %>% 
  filter(owner_count <= 8) %>% 
  group_by(owner_count) %>% 
  ggplot(aes(x = owner_count, y = price, group = owner_count)) +
  geom_boxplot(alpha = 0.6) +
  theme_minimal() +
  scale_y_log10() +
  labs(title = "Price vs. Owner Count", 
       x = "Number of Previous Owners", y = "Price ($)", 
       fill = "Is CPO")
#ggsave("plots/EDA_NumOwners_Price.png")
```

### Train and Test Split/Pre-Process Data

Training and Validation Data
```{r Train Test}
set.seed(222)
# Put 3/4 of the data into the training set 
data_split <- initial_split(data, prop = 3/4)

# Create data frames for the two sets:
other_data <- training(data_split)
test_data  <- testing(data_split)

val_data <- validation_split(other_data, 
                             prop = 0.7)
```

Create Recipe for LASSO 
```{r recipe}
recipe <- recipe(price ~ ., data = other_data) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors() - all_nominal())

summary(recipe)
```

### LASSO Regression
Add the LASSO to the workflow
```{r workflow}
lasso1 <- linear_reg(penalty = 0.1, mixture = 1) %>% # mixture = 1 -> lasso
  set_engine("glmnet")

workflow <- workflow() %>% 
  add_recipe(recipe)
```

#### Untuned LASSO
```{r}
lasso1_fit <- workflow %>% 
  add_model(lasso1) %>% 
  fit(data = other_data)

lasso1_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

lasso1_preds <- lasso1_fit %>% 
  augment(new_data = other_data)

lasso1_rsq <- rsq(lasso1_preds, price, .pred)$.estimate
```

#### Tuning $\lambda$ for LASSO

List of $\lambda$ values to test in CV
```{r penalty tuning LASSO}
lasso_2 <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

lambda_grid <- grid_regular(penalty(), levels = 100)
lambda_grid <- tibble(penalty = 10^seq(-4, 0, length.out = 40))

data_boot <- bootstraps(other_data)

doParallel::registerDoParallel()

lasso_grid <- tune_grid(
  workflow %>% 
    add_model(lasso_2),
  resamples = data_boot,
  grid = lambda_grid)

lasso_grid <- workflow %>% add_model(lasso_2) %>% 
  tune_grid(val_data,
            grid = lambda_grid,
            control = control_grid(save_pred = T, save_workflow = T),
            metrics = metric_set(rsq))

lasso_grid %>% 
  collect_metrics()
```

$\lambda$ selection
```{r Penalty tuning LASSO results}
lasso_grid %>% 
  collect_metrics() %>% 
  dplyr::filter(.metric == "rsq") %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  ylab("Explained Variance") +
  scale_x_log10(labels = scales::label_number()) + 
  ggtitle("Tuning Lasso Penalty") +
  theme_minimal() + theme_bw()
#ggsave("images/lasso_tuning.png")
```

```{r Select Best Penalty LASSO}
lasso_grid %>% 
  show_best("rsq", n = 15) %>% 
  arrange(-mean, penalty)

lr_best <- lasso_grid %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  slice(1)
```

### RIDGE Regression
Add the RIDGE to the workflow
```{r untrained ridge}
ridge1 <- linear_reg(penalty = 0.1, mixture = 0) %>% # mixture = 1 -> lasso
  set_engine("glmnet")
```

#### Untuned LASSO
```{r}
ridge1_fit <- workflow %>% 
  add_model(ridge1) %>% 
  fit(data = other_data)

ridge1_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

ridge1_preds <- ridge1_fit %>% 
  augment(new_data = other_data)

ridge1_rsq <- rsq(ridge1_preds, price, .pred)$.estimate
```

#### Tuning $\lambda$ for RIDGE

List of $\lambda$ values to test in CV
```{r penalty tuning RIDGE}
ridge_2 <- linear_reg(penalty = tune(), mixture = 0) %>% 
  set_engine("glmnet")

ridge_grid <- tibble(penalty = 10^seq(-4, 0, length.out = 40))

doParallel::registerDoParallel()

ridge_grid <- workflow %>% add_model(ridge_2) %>% 
  tune_grid(val_data,
            grid = ridge_grid,
            control = control_grid(save_pred = T, save_workflow = T),
            metrics = metric_set(rsq))

ridge_grid %>% 
  collect_metrics()
```

$\lambda$ selection
```{r Penalty tuning RIDGE results}
ridge_grid %>% 
  collect_metrics() %>% 
  dplyr::filter(.metric == "rsq") %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  ylab("Explained Variance") +
  scale_x_log10(labels = scales::label_number()) + 
  ggtitle("Tuning Ridge Penalty") +
  theme_minimal() + theme_bw()
#ggsave("images/lasso_tuning.png")
```

```{r Select Best Penalty RIDGE}
ridge_grid %>% 
  show_best("rsq", n = 15) %>% 
  arrange(-mean, penalty)

ridge_best <- ridge_grid %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  slice(1)
```

### Linear Regression - basic stuff :)
```{r lin reg test train split}
data1 = sort(sample(nrow(data), nrow(data)*.6))
#creating training data set by selecting the output row values
train<-data[data1,]
#creating test data set by not selecting the output row values
test<-data[-data1,]
```

```{r initial feature selection for simple lm}
# Using leaps package to create best subset on the training data
set.seed(1234)
Best_Subset <- regsubsets(price ~ .,
                          data = train,
                          nbest = 1,
                          nvmax = NULL,
                          force.in = NULL,
                          force.out = NULL,
                          method = "backward")

summary_best_subset <- summary(Best_Subset)

# find which one is the best subset
which.max(summary_best_subset$adjr2)

# print the variables we need
summary_best_subset$which[69,]

# basic linear regression on all variables
baseline <- lm(price ~ ., data = train)
summary(baseline)

# removing some observations with new factor levels
test1 <- test %>% filter(maximum_seating != "--",
                         transmission_display != "8-Speed CVT")
test <- test %>% filter(maximum_seating != "--",
                         transmission_display != "8-Speed CVT")

# getting predictions
preds <- predict(baseline, test1)
preds <- as.data.frame(preds)

# calculating error df
error <- preds %>% cbind(test$price) %>%
  mutate(diff = test$price - preds)
```

Osho's attempt at an intuitive model, it wasn't good
```{r osho intuitive model}
lm1 <- lm(price ~ year + mileage + has_accidents + transmission + city_fuel_economy + body_type, data = train)

summary(lm1)
```
Step AIC Feature Selection on the Baseline model
```{r stepAIC}
library(MASS)
lm_AIC <- stepAIC(baseline)
```

Model output
```{r step AIC results}
summary(lm_AIC) 
```

